{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nat/anaconda2/envs/mrcnn/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-736ce68e83ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mvisualize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_top_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Path:  /deepmatter/mask_rcnn/logs/shapes2017102802/mask_rcnn_{epoch:04d}.h5\n",
      "Starting at epoch 0. LR=0.002\n",
      "\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 73s - loss: 2.2164 - rpn_class_loss: 0.0242 - rpn_bbox_loss: 1.0638 - mrcnn_class_loss: 0.2426 - mrcnn_bbox_loss: 0.3006 - mrcnn_mask_loss: 0.2385 - val_loss: 1.8454 - val_rpn_class_loss: 0.0232 - val_rpn_bbox_loss: 0.9971 - val_mrcnn_class_loss: 0.1398 - val_mrcnn_bbox_loss: 0.1343 - val_mrcnn_mask_loss: 0.2042\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint Path:  /deepmatter/mask_rcnn/logs/shapes2017102802/mask_rcnn_{epoch:04d}.h5\n",
      "Starting at epoch 0. LR=0.0002\n",
      "\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:1987: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "100/100 [==============================] - 86s - loss: 11.4006 - rpn_class_loss: 0.0184 - rpn_bbox_loss: 0.8409 - mrcnn_class_loss: 0.1576 - mrcnn_bbox_loss: 0.0902 - mrcnn_mask_loss: 0.1977 - val_loss: 11.4376 - val_rpn_class_loss: 0.0220 - val_rpn_bbox_loss: 1.0068 - val_mrcnn_class_loss: 0.1172 - val_mrcnn_bbox_loss: 0.0683 - val_mrcnn_mask_loss: 0.1278\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-64c45e17617b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Either set a specific path or find last trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load trained weights (fill in path to trained weights here)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/6)/zSingh/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mfind_last\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2052\u001b[0m         \"\"\"\n\u001b[1;32m   2053\u001b[0m         \u001b[0;31m# Get directory names. Each directory corresponds to a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2054\u001b[0;31m         \u001b[0mdir_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2055\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m         \u001b[0mdir_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "print (gt_mask.type)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:  108.00000  max:  236.00000\n",
      "molded_images            shape: (1, 128, 128, 3)      min:  -15.70000  max:  132.10000\n",
      "image_metas              shape: (1, 12)               min:    0.00000  max:  128.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAHaCAYAAAAzAiFdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niokxABQTYwAo9j8Kt/MFNRKR6wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff36b5de080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.95\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
